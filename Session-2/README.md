# NERSC CUDA Tutorials Session 2

This tutorial session consists of two different parts. The first part will deal with launch configurations and go over the material studied during the presentation. The second part of this tutorial will give practical examples of memory coalescing and caching.

## Kernel Launch Configurations 
To study the launch configuration we will consider the __vector_add_kernel__ in the file __vec_add.cu__. This is a simple vector addition example where a vector __A__ and vector __B__ of same lengths are added and results are stored in a third vector __C__. The work load of this vector addition is distributed across all the available threads in all the CUDA blocks combined. For instance, if the total number of threads available is __1__ then the __for loop__ in this kernel will make total iterations equal to the length of vectors. If we increase the number of threads, the total iterations by this loop will decrease as more threads are utilized.

To get started, open the file __vec_add.cu__ and move the cursor to __line 51__. Here the variable __blocks__ is the number of blocks launched per kernel launch, and __threads__ is the number of threads launched per block. Set the __blocks__ to __1__ and __threads__ to __256__, this will launch one block with 256 threads. Save the changes and build this by using the Makefile in directory __Session-2__. Once it has been built, you can run the file by using the command in script __run.sh__ by typing _sh run.sh_ in the terminal and executing. __run.sh__ script executes the program with profiler and collects some metrics that are then displayed on your screen. For this first exercise, only consider the Duration and Memory Throughput.

Now increase the number of blocks to 4 and repeat the above steps to build and run the kernel. You will observe that the Memory Throughput will increase by 4 (approximately) and the Duration or runtime will be decreased by 4 (approximately). 

As a next step increase the total number of threads to the maximum possible value that we saw during the presentation session. You can do this setting number of blocks equal to 320 and number of threads equal to 512. This will launch the maximum number of threads that can reside on a V100 at a given time. Build and run to observe the memory throughput, this value should now be close to 900 Gb/second which is the peak value for V100.

## Global Memory Coalescing
To study global memory coalescing, we will make use of the kernel __vector_add_kernel_memory__ that can be found in same file __vec_add.cu__. Be sure to comment out the kernel launch from previous exercise by commenting out the line __59__ and uncomment the kernel launch of __vector_add_kernel_memory__ by uncommenting the lines __64__ and __65__. Kernel __vector_add_kernel_memory__ performs a vector addition similar to the previous one but with a memory stride being provided as an arguement to the kernel, this can be controlled by the variable __mem_stride__. For instance, if the variable __mem_stride__ is set to __1__ then all the threads will process elements contiguous to each, similarly when this variable is set to __8__ then threads will process elements located at a stride of __8__ or __8*4__ bytes apart.

To begin, set the __mem_stride__ variable to __8__ so that threads are accessing memory locations __32__ bytes apart. Build this by using the same __Makefile__ from previous exercise and run this by using the script __transactions.sh__. This script will gather two different metrics which represent the total number of global memory __requests__ _(l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum)_ and total global memory __transactions__ to serve those requests _(l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum)_. We can obtain the number of __transactions per request__ by dividing the total __transactions__ by the total __requests__. 

You will observe that with a stride of __8__ transactions per request come out to be __32__, that means a separate memory transaction for each thread of a warp which is no very desirable. As a second step, set the stride to __4__ and repeat the above steps, you will observe that the number of transactions per request are halved and the best results (least number of transactions per request) are possible when the stride is set to __1__.

As an optional exercise, you can repeat the above tests with the __run.sh__ script and observe how the __L1__ and __L2__ hit rate vary with the stride size and the impact on run time.